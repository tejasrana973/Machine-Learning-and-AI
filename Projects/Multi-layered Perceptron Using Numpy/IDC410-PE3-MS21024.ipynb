{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Defining OOP class for Hyper Parameters, Weight, Bias, Activation functions :**\n",
        "---\n",
        "---"
      ],
      "metadata": {
        "id": "JQ5blcYMWEMI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jgTuQkrskb0B"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "54Hveg9jkb0D"
      },
      "outputs": [],
      "source": [
        "class HyperParameters:\n",
        "    def __init__(self, learning_rate=0.01, epochs=10, mini_batch_size=None, beta=.9, layers=None, beta1=.9, beta2=.998, lambd=0):\n",
        "        if layers is None:\n",
        "            layers = [10, 20, 10]\n",
        "        self.layers = layers\n",
        "        self.no_l = len(layers)\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = mini_batch_size\n",
        "        self.beta = beta\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.lambd = lambd\n",
        "\n",
        "class WeightAndBias:\n",
        "    def __init__(self, number_features, layers, initialisation_type=\"random\", weights=None, biases=None):\n",
        "\n",
        "        self.initialisation_type = initialisation_type\n",
        "        self.layers = [number_features] + layers\n",
        "        if self.initialisation_type == \"random\":\n",
        "            self.weights = [pd.DataFrame()] + [np.random.randn(self.layers[i+1], self.layers[i]) * 0.01 for i in range(len(self.layers)-1)]\n",
        "            self.biases = [pd.DataFrame()] + [np.zeros([self.layers[i+1], 1]) for i in range(len(self.layers)-1)]\n",
        "        elif self.initialisation_type == \"manual\":\n",
        "            self.weights = [pd.DataFrame()] + weights\n",
        "            self.biases = [pd.DataFrame()] + biases\n",
        "\n",
        "    def update_learning_parameters(self, no_l, hp_obj, dW, db, m_training) :\n",
        "        for l in range(1, no_l+1):\n",
        "            self.biases[l] =  self.biases[l] - hp_obj.learning_rate * db[l]\n",
        "            self.weights[l] = (1 - (hp_obj.lambd *  hp_obj.learning_rate)/m_training) * self.weights[l] - hp_obj.learning_rate * dW[l]\n",
        "\n",
        "class ActivationFunctions:\n",
        "    def __init__(self, layers, activation_functions=None) :\n",
        "        if activation_functions is None:\n",
        "            activation_functions= ['tanh'] * (len(layers) - 1) + ['softmax']\n",
        "\n",
        "        self.activation_functions = [None] + [eval(f'ActivationFunctions.{activation_function}')\n",
        "                                     for activation_function in activation_functions]\n",
        "\n",
        "        self.derivative_functions = [None] + [eval(f'ActivationFunctions.{activation_function}_derivative')\n",
        "                                     for activation_function in activation_functions]\n",
        "\n",
        "    @staticmethod\n",
        "    def sigmoid(z) :\n",
        "        return 1 / (1 + np.exp( -z ))\n",
        "\n",
        "    @staticmethod\n",
        "    def relu(z) :\n",
        "        return np.where(z>0, z, 0.0001 * z )\n",
        "\n",
        "    @staticmethod\n",
        "    def tanh(z) :\n",
        "        # return np.tanh(z\n",
        "        z = np.clip(z, -20, 20)\n",
        "        return (np.exp(z) - np.exp(-z))/ (np.exp(z) + np.exp(-z))\n",
        "\n",
        "    @staticmethod\n",
        "    def softmax(z):\n",
        "        z = np.clip(z, -20, 20)\n",
        "        return np.exp(z) / np.sum(np.exp(z), axis=0)\n",
        "\n",
        "    @staticmethod\n",
        "    def softmax_derivative(y, a) :\n",
        "        return a - y\n",
        "\n",
        "    @staticmethod\n",
        "    def sigmoid_derivative(z) :\n",
        "        return  (1 / (1 + np.exp( -z ))) * (1 - ( 1 / (1 + np.exp( -z ))))\n",
        "\n",
        "    @staticmethod\n",
        "    def tanh_derivative(z) :\n",
        "        return (1 - np.tanh(z) ** 2)\n",
        "\n",
        "    @staticmethod\n",
        "    def relu_derivative(z) :\n",
        "        return (z > 0) * 1\n",
        "\n",
        "    @staticmethod\n",
        "    def calculate_loss(a, y, m, hp, lp) :\n",
        "            return (-1/m * np.sum(np.multiply(y, np.log(a))), -1/m * np.sum(np.multiply(y, np.log(a))) + hp.lambd/(2 *m ) * sum(np.sum(np.square(lp.weights[i])) for i in range(1, hp.no_l+1)))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Defining OOP class for Neural Network :**\n",
        "---\n",
        "---"
      ],
      "metadata": {
        "id": "CJfAlzCxW6jx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "84xrucPckb0D"
      },
      "outputs": [],
      "source": [
        "class NeuralNetwork:\n",
        "    def __init__(self, X_train, y_train, HyperParameters, activation_functions=None) :\n",
        "\n",
        "        self.X_train, self.y_train = X_train, y_train\n",
        "        self.n, self.m = X_train.shape\n",
        "\n",
        "        print(f\"number of training examples: {self.m}\\nnumber of features: {self.n}\"\n",
        "              f\"\\nshape of y_train {self.y_train.shape}\")\n",
        "\n",
        "        #hp --> hyperparameters\n",
        "        self.hp = HyperParameters\n",
        "        self.layers = self.hp.layers\n",
        "        self.no_l = self.hp.no_l\n",
        "\n",
        "        self.act_function_obj = ActivationFunctions(self.layers, activation_functions=activation_functions)\n",
        "\n",
        "        #lp --> learning parameters -> weights and biases\n",
        "        self.lp = WeightAndBias(self.n, self.layers)\n",
        "\n",
        "        if self.hp.batch_size is None:\n",
        "            self.hp.batch_size = self.m\n",
        "\n",
        "\n",
        "    def forward_propagation(self, X_batch) :\n",
        "        self.Z, self.A = [0] + [None] * self.no_l, [X_batch ] + [None] * self.no_l\n",
        "        activation_functions = self.act_function_obj.activation_functions\n",
        "\n",
        "        for l in range(1, self.no_l + 1):\n",
        "            self.Z[l] = np.dot(self.lp.weights[l], self.A[l-1]) + self.lp.biases[l]\n",
        "            self.A[l] = activation_functions[l](self.Z[l])\n",
        "\n",
        "    def back_propagation(self, y_batch) :\n",
        "\n",
        "        derivative_functions = self.act_function_obj.derivative_functions\n",
        "        batch_size = y_batch.shape[1]\n",
        "\n",
        "        self.dZ =[None] +  [None] * self.no_l\n",
        "        self.dW =[None] +  [None] * self.no_l\n",
        "        self.db =[None] +  [None] * self.no_l\n",
        "\n",
        "        self.dZ[self.no_l] = derivative_functions[self.no_l](y_batch, self.A[self.no_l])\n",
        "        self.dW[self.no_l] = 1/batch_size * np.dot(self.dZ[self.no_l] , self.A[self.no_l - 1].T)\n",
        "        self.db[self.no_l] = 1/batch_size * np.sum(self.dZ[self.no_l], axis=1, keepdims=True)\n",
        "\n",
        "        assert self.dZ[self.no_l].shape == self.Z[self.no_l].shape\n",
        "        assert self.db[self.no_l].shape == self.lp.biases[self.no_l].shape\n",
        "        assert self.dW[self.no_l].shape == self.lp.weights[self.no_l].shape\n",
        "\n",
        "        for l in range(self.no_l - 1, 0, -1) :\n",
        "\n",
        "            self.dZ[l] = np.dot(self.lp.weights[l+1].T, self.dZ[l+1] )* derivative_functions[l](self.Z[l])\n",
        "            self.dW[l] = 1/batch_size * np.dot(self.dZ[l], self.A[l-1].T)\n",
        "            self.db[l] = 1/batch_size * np.sum(self.dZ[l], axis=1, keepdims=True)\n",
        "\n",
        "            assert self.dZ[l].shape == self.Z[l].shape\n",
        "            assert self.dW[l].shape == self.lp.weights[l].shape\n",
        "            assert self.db[l].shape == self.lp.biases[l].shape\n",
        "\n",
        "\n",
        "    def train_nn(self, verbose=False, per_epoch_log=100) :\n",
        "        for epoch in range(self.hp.epochs):\n",
        "            for batch_s in range(0, self.m, self.hp.batch_size) :\n",
        "\n",
        "                batch_e = min(batch_s + self.hp.batch_size, self.m)\n",
        "\n",
        "                X_batch = self.X_train[:, batch_s: batch_e]\n",
        "                y_batch = self.y_train[:, batch_s: batch_e]\n",
        "                m_batch_size = batch_e - batch_s\n",
        "\n",
        "                self.forward_propagation(X_batch)\n",
        "                self.back_propagation(y_batch)\n",
        "                self.lp.update_learning_parameters(self.no_l, self.hp,  self.dW, self.db, m_batch_size)\n",
        "\n",
        "            if verbose and epoch % per_epoch_log == 0:\n",
        "                print(f\"epochs {epoch} loss: \",ActivationFunctions.calculate_loss(self.A[self.no_l], y_batch, m_batch_size, self.hp, self.lp))\n",
        "\n",
        "    def predict(self, X_test):\n",
        "        self.forward_propagation(X_test)\n",
        "        preds=  self.A[self.no_l].T\n",
        "        return (preds == preds.max(axis=1)[:,None]).astype(int)\n",
        "\n",
        "def one_hot_encode(labels, num_classes):\n",
        "    return np.eye(num_classes)[labels.reshape(-1)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8CiKY7rkb0E"
      },
      "source": [
        "**Answer-1**\n",
        "---\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "7gd6zdyCkb0E"
      },
      "outputs": [],
      "source": [
        "def data_reader(path):\n",
        "    data = pd.read_csv(path)\n",
        "    X = np.array(data.drop([\"label\"], axis=1)) / 255.0 # normalizing the data to prevent overflow in np.exp\n",
        "    Y = np.array(data.label).reshape(-1, 1)\n",
        "    return X, Y\n",
        "\n",
        "def train_test_split(X, Y):\n",
        "    train_X = X[0:50000, :]\n",
        "    test_X = X[50000:60000, :]\n",
        "    train_Y = Y[0:50000, :]\n",
        "    test_Y = Y[50000:60000, :]\n",
        "    return train_X, train_Y, test_X, test_Y\n",
        "\n",
        "X, Y = data_reader(\"mnist_train.csv\")\n",
        "train_X, train_Y, test_X, test_Y = train_test_split(X,Y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9UfrFv1Hkb0E"
      },
      "source": [
        "**Answer-2**\n",
        "---\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LwY6AvWokb0F",
        "outputId": "fab21f1c-7eac-4817-dac7-1b95e6d5bf94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of training examples: 50000\n",
            "number of features: 784\n",
            "shape of y_train (10, 50000)\n",
            "epochs 0 loss:  (2.3025850929940463, 2.3025850929940463)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([Empty DataFrame\n",
              "  Columns: []\n",
              "  Index: [],\n",
              "  array([[1., 1., 1., ..., 1., 1., 1.],\n",
              "         [1., 1., 1., ..., 1., 1., 1.],\n",
              "         [1., 1., 1., ..., 1., 1., 1.],\n",
              "         ...,\n",
              "         [1., 1., 1., ..., 1., 1., 1.],\n",
              "         [1., 1., 1., ..., 1., 1., 1.],\n",
              "         [1., 1., 1., ..., 1., 1., 1.]])],\n",
              " [Empty DataFrame\n",
              "  Columns: []\n",
              "  Index: [],\n",
              "  array([[0.99932],\n",
              "         [1.00678],\n",
              "         [0.99968],\n",
              "         [1.00101],\n",
              "         [0.99859],\n",
              "         [0.99506],\n",
              "         [0.99951],\n",
              "         [1.00175],\n",
              "         [0.99842],\n",
              "         [0.99988]])])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "def zero_hidden_network(X, Y, weight_matrix, bias_matrix, learning_rate):\n",
        "    m = X.shape[0]\n",
        "    y = one_hot_encode(Y, num_classes=10)\n",
        "\n",
        "    layers = [10]\n",
        "    activation_functions = ['softmax']\n",
        "    hp = HyperParameters(layers=layers, learning_rate=learning_rate, epochs=1)\n",
        "    nn = NeuralNetwork(X.T, y.T, hp, activation_functions=activation_functions)\n",
        "\n",
        "    nn.lp = WeightAndBias(nn.n, nn.layers, initialisation_type=\"manual\", weights=weight_matrix, biases=bias_matrix)\n",
        "    nn.train_nn(verbose=True, per_epoch_log=1)\n",
        "\n",
        "    return nn.lp.weights, nn.lp.biases\n",
        "\n",
        "weights = np.ones((10, 784))\n",
        "biases = np.ones((10, 1))\n",
        "\n",
        "zero_hidden_network(train_X, train_Y, [weights], [biases], learning_rate=0.5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jy2327c8kb0F"
      },
      "source": [
        "**Answer-3**\n",
        "---\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JBhuAg71kb0F",
        "outputId": "67164298-b46b-4a96-ecbe-424d93bb05c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of training examples: 50000\n",
            "number of features: 784\n",
            "shape of y_train (10, 50000)\n",
            "epochs 0 loss:  (2.3025850929940463, 2.3025850929940463)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([Empty DataFrame\n",
              "  Columns: []\n",
              "  Index: [],\n",
              "  array([[1., 1., 1., ..., 1., 1., 1.],\n",
              "         [1., 1., 1., ..., 1., 1., 1.],\n",
              "         [1., 1., 1., ..., 1., 1., 1.],\n",
              "         ...,\n",
              "         [1., 1., 1., ..., 1., 1., 1.],\n",
              "         [1., 1., 1., ..., 1., 1., 1.],\n",
              "         [1., 1., 1., ..., 1., 1., 1.]]),\n",
              "  array([[0.99932, 0.99932, 0.99932, 0.99932, 0.99932, 0.99932, 0.99932,\n",
              "          0.99932, 0.99932, 0.99932],\n",
              "         [1.00678, 1.00678, 1.00678, 1.00678, 1.00678, 1.00678, 1.00678,\n",
              "          1.00678, 1.00678, 1.00678],\n",
              "         [0.99968, 0.99968, 0.99968, 0.99968, 0.99968, 0.99968, 0.99968,\n",
              "          0.99968, 0.99968, 0.99968],\n",
              "         [1.00101, 1.00101, 1.00101, 1.00101, 1.00101, 1.00101, 1.00101,\n",
              "          1.00101, 1.00101, 1.00101],\n",
              "         [0.99859, 0.99859, 0.99859, 0.99859, 0.99859, 0.99859, 0.99859,\n",
              "          0.99859, 0.99859, 0.99859],\n",
              "         [0.99506, 0.99506, 0.99506, 0.99506, 0.99506, 0.99506, 0.99506,\n",
              "          0.99506, 0.99506, 0.99506],\n",
              "         [0.99951, 0.99951, 0.99951, 0.99951, 0.99951, 0.99951, 0.99951,\n",
              "          0.99951, 0.99951, 0.99951],\n",
              "         [1.00175, 1.00175, 1.00175, 1.00175, 1.00175, 1.00175, 1.00175,\n",
              "          1.00175, 1.00175, 1.00175],\n",
              "         [0.99842, 0.99842, 0.99842, 0.99842, 0.99842, 0.99842, 0.99842,\n",
              "          0.99842, 0.99842, 0.99842],\n",
              "         [0.99988, 0.99988, 0.99988, 0.99988, 0.99988, 0.99988, 0.99988,\n",
              "          0.99988, 0.99988, 0.99988]])],\n",
              " [Empty DataFrame\n",
              "  Columns: []\n",
              "  Index: [],\n",
              "  array([[1.],\n",
              "         [1.],\n",
              "         [1.],\n",
              "         [1.],\n",
              "         [1.],\n",
              "         [1.],\n",
              "         [1.],\n",
              "         [1.],\n",
              "         [1.],\n",
              "         [1.]]),\n",
              "  array([[0.99932],\n",
              "         [1.00678],\n",
              "         [0.99968],\n",
              "         [1.00101],\n",
              "         [0.99859],\n",
              "         [0.99506],\n",
              "         [0.99951],\n",
              "         [1.00175],\n",
              "         [0.99842],\n",
              "         [0.99988]])])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "def one_hidden_layer_network(X, Y, weight_matrices, bias_vectors, learning_rate):\n",
        "    m = X.shape[0]\n",
        "    y = one_hot_encode(Y, num_classes=10)\n",
        "\n",
        "    layers = [10, 10]\n",
        "    activation_functions = ['sigmoid', 'softmax']\n",
        "    hp = HyperParameters(layers=layers, learning_rate=learning_rate, epochs=1)\n",
        "    nn = NeuralNetwork(X.T, y.T, hp, activation_functions=activation_functions)\n",
        "\n",
        "    nn.lp = WeightAndBias(nn.n, nn.layers, initialisation_type=\"manual\", weights=weight_matrices, biases=bias_vectors)\n",
        "    nn.train_nn(verbose=True, per_epoch_log=1)\n",
        "\n",
        "    return nn.lp.weights, nn.lp.biases\n",
        "\n",
        "weights = [np.ones((10, 784)), np.ones((10, 10))]\n",
        "biases = [np.ones((10, 1)), np.ones((10, 1))]\n",
        "one_hidden_layer_network(train_X, train_Y, weight_matrices=weights, bias_vectors=biases, learning_rate=0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUSzLjrdkb0G"
      },
      "source": [
        "**Answer-4**\n",
        "---\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HnePAM6nkb0G",
        "outputId": "a9b822ba-b9f7-42f5-b7ca-aa9a212765d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of training examples: 50000\n",
            "number of features: 784\n",
            "shape of y_train (10, 50000)\n",
            "epochs 0 loss:  (2.3025850929940463, 2.3025850929940463)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([Empty DataFrame\n",
              "  Columns: []\n",
              "  Index: [],\n",
              "  array([[1., 1., 1., ..., 1., 1., 1.],\n",
              "         [1., 1., 1., ..., 1., 1., 1.],\n",
              "         [1., 1., 1., ..., 1., 1., 1.],\n",
              "         ...,\n",
              "         [1., 1., 1., ..., 1., 1., 1.],\n",
              "         [1., 1., 1., ..., 1., 1., 1.],\n",
              "         [1., 1., 1., ..., 1., 1., 1.]]),\n",
              "  array([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]),\n",
              "  array([[0.99932001, 0.99932001, 0.99932001, 0.99932001, 0.99932001,\n",
              "          0.99932001, 0.99932001, 0.99932001, 0.99932001, 0.99932001],\n",
              "         [1.00677989, 1.00677989, 1.00677989, 1.00677989, 1.00677989,\n",
              "          1.00677989, 1.00677989, 1.00677989, 1.00677989, 1.00677989],\n",
              "         [0.99968001, 0.99968001, 0.99968001, 0.99968001, 0.99968001,\n",
              "          0.99968001, 0.99968001, 0.99968001, 0.99968001, 0.99968001],\n",
              "         [1.00100998, 1.00100998, 1.00100998, 1.00100998, 1.00100998,\n",
              "          1.00100998, 1.00100998, 1.00100998, 1.00100998, 1.00100998],\n",
              "         [0.99859002, 0.99859002, 0.99859002, 0.99859002, 0.99859002,\n",
              "          0.99859002, 0.99859002, 0.99859002, 0.99859002, 0.99859002],\n",
              "         [0.99506008, 0.99506008, 0.99506008, 0.99506008, 0.99506008,\n",
              "          0.99506008, 0.99506008, 0.99506008, 0.99506008, 0.99506008],\n",
              "         [0.99951001, 0.99951001, 0.99951001, 0.99951001, 0.99951001,\n",
              "          0.99951001, 0.99951001, 0.99951001, 0.99951001, 0.99951001],\n",
              "         [1.00174997, 1.00174997, 1.00174997, 1.00174997, 1.00174997,\n",
              "          1.00174997, 1.00174997, 1.00174997, 1.00174997, 1.00174997],\n",
              "         [0.99842003, 0.99842003, 0.99842003, 0.99842003, 0.99842003,\n",
              "          0.99842003, 0.99842003, 0.99842003, 0.99842003, 0.99842003],\n",
              "         [0.99988   , 0.99988   , 0.99988   , 0.99988   , 0.99988   ,\n",
              "          0.99988   , 0.99988   , 0.99988   , 0.99988   , 0.99988   ]])],\n",
              " [Empty DataFrame\n",
              "  Columns: []\n",
              "  Index: [],\n",
              "  array([[1.],\n",
              "         [1.],\n",
              "         [1.],\n",
              "         [1.],\n",
              "         [1.],\n",
              "         [1.],\n",
              "         [1.],\n",
              "         [1.],\n",
              "         [1.],\n",
              "         [1.]]),\n",
              "  array([[1.],\n",
              "         [1.],\n",
              "         [1.],\n",
              "         [1.],\n",
              "         [1.],\n",
              "         [1.],\n",
              "         [1.],\n",
              "         [1.],\n",
              "         [1.],\n",
              "         [1.]]),\n",
              "  array([[0.99932],\n",
              "         [1.00678],\n",
              "         [0.99968],\n",
              "         [1.00101],\n",
              "         [0.99859],\n",
              "         [0.99506],\n",
              "         [0.99951],\n",
              "         [1.00175],\n",
              "         [0.99842],\n",
              "         [0.99988]])])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "def n_hidden_layer_network(X, Y, weight_matrices, bias_vectors, learning_rate):\n",
        "    m = X.shape[0]\n",
        "    y = one_hot_encode(Y, num_classes=10)\n",
        "\n",
        "    layers = [10, 10, 10]\n",
        "    activation_functions = ['sigmoid', 'sigmoid', 'softmax']\n",
        "    hp = HyperParameters(layers=layers, learning_rate=learning_rate, epochs=1)\n",
        "    nn = NeuralNetwork(X.T, y.T, hp, activation_functions=activation_functions)\n",
        "\n",
        "    nn.lp = WeightAndBias(nn.n, nn.layers, initialisation_type=\"manual\", weights=weight_matrices, biases=bias_vectors)\n",
        "    nn.train_nn(verbose=True, per_epoch_log=1)\n",
        "\n",
        "    return nn.lp.weights, nn.lp.biases\n",
        "\n",
        "weights = [np.ones((10, 784)), np.ones((10, 10)), np.ones((10, 10))]\n",
        "biases = [np.ones((10, 1)), np.ones((10, 1)), np.ones((10, 1))]\n",
        "\n",
        "n_hidden_layer_network(train_X, train_Y, weights, biases, learning_rate=0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6O2Tiafkb0G"
      },
      "source": [
        "**Answer-5**\n",
        "---\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WEv2bfrekb0G",
        "outputId": "a4f30208-67ca-461d-a2c4-78709ddfb6da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of training examples: 50000\n",
            "number of features: 784\n",
            "shape of y_train (10, 50000)\n",
            "epochs 0 loss:  (2.3025850929940463, 2.3025850929940463)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([Empty DataFrame\n",
              "  Columns: []\n",
              "  Index: [],\n",
              "  array([[1., 1., 1., ..., 1., 1., 1.],\n",
              "         [1., 1., 1., ..., 1., 1., 1.],\n",
              "         [1., 1., 1., ..., 1., 1., 1.],\n",
              "         ...,\n",
              "         [1., 1., 1., ..., 1., 1., 1.],\n",
              "         [1., 1., 1., ..., 1., 1., 1.],\n",
              "         [1., 1., 1., ..., 1., 1., 1.]]),\n",
              "  array([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]),\n",
              "  array([[0.99932, 0.99932, 0.99932, 0.99932, 0.99932, 0.99932, 0.99932,\n",
              "          0.99932, 0.99932, 0.99932],\n",
              "         [1.00678, 1.00678, 1.00678, 1.00678, 1.00678, 1.00678, 1.00678,\n",
              "          1.00678, 1.00678, 1.00678],\n",
              "         [0.99968, 0.99968, 0.99968, 0.99968, 0.99968, 0.99968, 0.99968,\n",
              "          0.99968, 0.99968, 0.99968],\n",
              "         [1.00101, 1.00101, 1.00101, 1.00101, 1.00101, 1.00101, 1.00101,\n",
              "          1.00101, 1.00101, 1.00101],\n",
              "         [0.99859, 0.99859, 0.99859, 0.99859, 0.99859, 0.99859, 0.99859,\n",
              "          0.99859, 0.99859, 0.99859],\n",
              "         [0.99506, 0.99506, 0.99506, 0.99506, 0.99506, 0.99506, 0.99506,\n",
              "          0.99506, 0.99506, 0.99506],\n",
              "         [0.99951, 0.99951, 0.99951, 0.99951, 0.99951, 0.99951, 0.99951,\n",
              "          0.99951, 0.99951, 0.99951],\n",
              "         [1.00175, 1.00175, 1.00175, 1.00175, 1.00175, 1.00175, 1.00175,\n",
              "          1.00175, 1.00175, 1.00175],\n",
              "         [0.99842, 0.99842, 0.99842, 0.99842, 0.99842, 0.99842, 0.99842,\n",
              "          0.99842, 0.99842, 0.99842],\n",
              "         [0.99988, 0.99988, 0.99988, 0.99988, 0.99988, 0.99988, 0.99988,\n",
              "          0.99988, 0.99988, 0.99988]])],\n",
              " [Empty DataFrame\n",
              "  Columns: []\n",
              "  Index: [],\n",
              "  array([[1.],\n",
              "         [1.],\n",
              "         [1.],\n",
              "         [1.],\n",
              "         [1.],\n",
              "         [1.],\n",
              "         [1.],\n",
              "         [1.],\n",
              "         [1.],\n",
              "         [1.]]),\n",
              "  array([[1.],\n",
              "         [1.],\n",
              "         [1.],\n",
              "         [1.],\n",
              "         [1.],\n",
              "         [1.],\n",
              "         [1.],\n",
              "         [1.],\n",
              "         [1.],\n",
              "         [1.]]),\n",
              "  array([[0.99932],\n",
              "         [1.00678],\n",
              "         [0.99968],\n",
              "         [1.00101],\n",
              "         [0.99859],\n",
              "         [0.99506],\n",
              "         [0.99951],\n",
              "         [1.00175],\n",
              "         [0.99842],\n",
              "         [0.99988]])])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "def n_hidden_layer_network_with_different_activations(X, Y, weight_matrices, bias_vectors, learning_rate, activations = None):\n",
        "    m = X.shape[0]\n",
        "    y = one_hot_encode(Y, num_classes=10)\n",
        "\n",
        "    layers = [10, 10, 10]\n",
        "    activation_functions = activations + ['softmax']\n",
        "    hp = HyperParameters(layers=layers, learning_rate=learning_rate, epochs=1)\n",
        "    nn = NeuralNetwork(X.T, y.T, hp, activation_functions=activation_functions)\n",
        "\n",
        "    nn.lp = WeightAndBias(nn.n, nn.layers, initialisation_type=\"manual\", weights=weight_matrices, biases=bias_vectors)\n",
        "    nn.train_nn(verbose=True, per_epoch_log=1)\n",
        "\n",
        "    return nn.lp.weights, nn.lp.biases\n",
        "\n",
        "weights = [np.ones((10, 784)), np.ones((10, 10)), np.ones((10, 10))]\n",
        "biases = [np.ones((10, 1)), np.ones((10, 1)), np.ones((10, 1))]\n",
        "\n",
        "n_hidden_layer_network_with_different_activations(train_X, train_Y, weights, biases, learning_rate=0.5, activations=['relu', 'sigmoid'])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JhWGXSRPkb0H"
      },
      "source": [
        "**Answer-6**\n",
        "---\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RY68pglZkb0H",
        "outputId": "99305a91-e5a3-4218-e618-3d09b7704032"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of training examples: 50000\n",
            "number of features: 784\n",
            "shape of y_train (10, 50000)\n",
            "epochs 0 loss:  (2.3025850929940463, 2.3749437976981636)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([Empty DataFrame\n",
              "  Columns: []\n",
              "  Index: [],\n",
              "  array([[0.999991, 0.999991, 0.999991, ..., 0.999991, 0.999991, 0.999991],\n",
              "         [0.999991, 0.999991, 0.999991, ..., 0.999991, 0.999991, 0.999991],\n",
              "         [0.999991, 0.999991, 0.999991, ..., 0.999991, 0.999991, 0.999991],\n",
              "         ...,\n",
              "         [0.999991, 0.999991, 0.999991, ..., 0.999991, 0.999991, 0.999991],\n",
              "         [0.999991, 0.999991, 0.999991, ..., 0.999991, 0.999991, 0.999991],\n",
              "         [0.999991, 0.999991, 0.999991, ..., 0.999991, 0.999991, 0.999991]]),\n",
              "  array([[0.999991, 0.999991, 0.999991, 0.999991, 0.999991, 0.999991,\n",
              "          0.999991, 0.999991, 0.999991, 0.999991],\n",
              "         [0.999991, 0.999991, 0.999991, 0.999991, 0.999991, 0.999991,\n",
              "          0.999991, 0.999991, 0.999991, 0.999991],\n",
              "         [0.999991, 0.999991, 0.999991, 0.999991, 0.999991, 0.999991,\n",
              "          0.999991, 0.999991, 0.999991, 0.999991],\n",
              "         [0.999991, 0.999991, 0.999991, 0.999991, 0.999991, 0.999991,\n",
              "          0.999991, 0.999991, 0.999991, 0.999991],\n",
              "         [0.999991, 0.999991, 0.999991, 0.999991, 0.999991, 0.999991,\n",
              "          0.999991, 0.999991, 0.999991, 0.999991],\n",
              "         [0.999991, 0.999991, 0.999991, 0.999991, 0.999991, 0.999991,\n",
              "          0.999991, 0.999991, 0.999991, 0.999991],\n",
              "         [0.999991, 0.999991, 0.999991, 0.999991, 0.999991, 0.999991,\n",
              "          0.999991, 0.999991, 0.999991, 0.999991],\n",
              "         [0.999991, 0.999991, 0.999991, 0.999991, 0.999991, 0.999991,\n",
              "          0.999991, 0.999991, 0.999991, 0.999991],\n",
              "         [0.999991, 0.999991, 0.999991, 0.999991, 0.999991, 0.999991,\n",
              "          0.999991, 0.999991, 0.999991, 0.999991],\n",
              "         [0.999991, 0.999991, 0.999991, 0.999991, 0.999991, 0.999991,\n",
              "          0.999991, 0.999991, 0.999991, 0.999991]]),\n",
              "  array([[0.999311, 0.999311, 0.999311, 0.999311, 0.999311, 0.999311,\n",
              "          0.999311, 0.999311, 0.999311, 0.999311],\n",
              "         [1.006771, 1.006771, 1.006771, 1.006771, 1.006771, 1.006771,\n",
              "          1.006771, 1.006771, 1.006771, 1.006771],\n",
              "         [0.999671, 0.999671, 0.999671, 0.999671, 0.999671, 0.999671,\n",
              "          0.999671, 0.999671, 0.999671, 0.999671],\n",
              "         [1.001001, 1.001001, 1.001001, 1.001001, 1.001001, 1.001001,\n",
              "          1.001001, 1.001001, 1.001001, 1.001001],\n",
              "         [0.998581, 0.998581, 0.998581, 0.998581, 0.998581, 0.998581,\n",
              "          0.998581, 0.998581, 0.998581, 0.998581],\n",
              "         [0.995051, 0.995051, 0.995051, 0.995051, 0.995051, 0.995051,\n",
              "          0.995051, 0.995051, 0.995051, 0.995051],\n",
              "         [0.999501, 0.999501, 0.999501, 0.999501, 0.999501, 0.999501,\n",
              "          0.999501, 0.999501, 0.999501, 0.999501],\n",
              "         [1.001741, 1.001741, 1.001741, 1.001741, 1.001741, 1.001741,\n",
              "          1.001741, 1.001741, 1.001741, 1.001741],\n",
              "         [0.998411, 0.998411, 0.998411, 0.998411, 0.998411, 0.998411,\n",
              "          0.998411, 0.998411, 0.998411, 0.998411],\n",
              "         [0.999871, 0.999871, 0.999871, 0.999871, 0.999871, 0.999871,\n",
              "          0.999871, 0.999871, 0.999871, 0.999871]])],\n",
              " [Empty DataFrame\n",
              "  Columns: []\n",
              "  Index: [],\n",
              "  array([[1.],\n",
              "         [1.],\n",
              "         [1.],\n",
              "         [1.],\n",
              "         [1.],\n",
              "         [1.],\n",
              "         [1.],\n",
              "         [1.],\n",
              "         [1.],\n",
              "         [1.]]),\n",
              "  array([[1.],\n",
              "         [1.],\n",
              "         [1.],\n",
              "         [1.],\n",
              "         [1.],\n",
              "         [1.],\n",
              "         [1.],\n",
              "         [1.],\n",
              "         [1.],\n",
              "         [1.]]),\n",
              "  array([[0.99932],\n",
              "         [1.00678],\n",
              "         [0.99968],\n",
              "         [1.00101],\n",
              "         [0.99859],\n",
              "         [0.99506],\n",
              "         [0.99951],\n",
              "         [1.00175],\n",
              "         [0.99842],\n",
              "         [0.99988]])])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "def n_hidden_layer_network_with_different_activations_with_momentum(X, Y, weight_matrices, bias_vectors, learning_rate, activations, momentum):\n",
        "    m = X.shape[0]\n",
        "    y = one_hot_encode(Y, num_classes=10)\n",
        "\n",
        "    layers = [10, 10, 10]\n",
        "    activation_functions = activations + ['softmax']\n",
        "    hp = HyperParameters(layers=layers, learning_rate=learning_rate, epochs=1, lambd=momentum)\n",
        "    nn = NeuralNetwork(X.T, y.T, hp, activation_functions=activation_functions)\n",
        "\n",
        "    nn.lp = WeightAndBias(nn.n, nn.layers, initialisation_type=\"manual\", weights=weight_matrices, biases=bias_vectors)\n",
        "    nn.train_nn(verbose=True, per_epoch_log=1)\n",
        "\n",
        "    return nn.lp.weights, nn.lp.biases\n",
        "\n",
        "weights = [np.ones((10, 784)), np.ones((10, 10)), np.ones((10, 10))]\n",
        "biases = [np.ones((10, 1)), np.ones((10, 1)), np.ones((10, 1))]\n",
        "n_hidden_layer_network_with_different_activations_with_momentum(train_X, train_Y, weights, biases, learning_rate=0.5, activations=['relu', 'sigmoid'], momentum=0.9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQswOkmrkb0H"
      },
      "source": [
        "**Full Training on the MNIST data :**\n",
        "---\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Bgi-9lgkb0H",
        "outputId": "aad17c71-3d89-4b7e-db0c-a4d4fd04bc03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of training examples: 50000\n",
            "number of features: 784\n",
            "shape of y_train (10, 50000)\n",
            "epochs 0 loss:  (2.301991806586728, 2.302686457384858)\n",
            "epochs 10 loss:  (2.1296114760823146, 2.1304435337149843)\n",
            "epochs 20 loss:  (0.5920577935553815, 0.5948145000653382)\n",
            "epochs 30 loss:  (0.4593165458278102, 0.46273964034121373)\n",
            "epochs 40 loss:  (0.38272657989650166, 0.3866551688378101)\n",
            "epochs 50 loss:  (0.31800784663937504, 0.32244456138854843)\n",
            "epochs 60 loss:  (0.26604525139636676, 0.27097931136857667)\n",
            "epochs 70 loss:  (0.22509320331415628, 0.2305137165850732)\n",
            "epochs 80 loss:  (0.19502015352369334, 0.2009041024757114)\n",
            "epochs 90 loss:  (0.170316661101825, 0.17664178519767426)\n",
            "epochs 100 loss:  (0.14937101154373533, 0.15611714075866304)\n",
            "epochs 110 loss:  (0.1317975277631281, 0.13894915143701253)\n",
            "epochs 120 loss:  (0.11672276252018518, 0.12426671004961781)\n",
            "epochs 130 loss:  (0.10371647946054009, 0.11164050990300521)\n",
            "epochs 140 loss:  (0.09243151069399942, 0.10072291245576817)\n",
            "epochs 150 loss:  (0.0824237802731485, 0.09107169871712009)\n",
            "epochs 160 loss:  (0.07340604006191265, 0.0824019121898977)\n",
            "epochs 170 loss:  (0.06530943058725674, 0.07464487763441569)\n",
            "epochs 180 loss:  (0.05807899800486145, 0.06774637216740947)\n",
            "epochs 190 loss:  (0.051918199929738874, 0.0619101872769959)\n",
            "epochs 200 loss:  (0.0462620469246136, 0.05657199304101825)\n",
            "epochs 210 loss:  (0.04124907256002236, 0.051870553799157025)\n",
            "epochs 220 loss:  (0.036802661237335374, 0.04772958924239086)\n",
            "epochs 230 loss:  (0.03297181362482908, 0.04419808478122328)\n",
            "epochs 240 loss:  (0.02978649706684499, 0.04130668634741178)\n",
            "epochs 250 loss:  (0.026967344814918793, 0.03877654625609138)\n",
            "epochs 260 loss:  (0.024405315971259882, 0.03649936485275267)\n",
            "epochs 270 loss:  (0.022264742891902348, 0.03464032820154868)\n",
            "epochs 280 loss:  (0.020307618866244104, 0.03296227450968717)\n",
            "epochs 290 loss:  (0.018625756120176504, 0.031558163734113145)\n",
            "epochs 300 loss:  (0.017239233132329272, 0.030450024702289365)\n",
            "epochs 310 loss:  (0.016045354760249146, 0.029537421476268)\n",
            "epochs 320 loss:  (0.0150652519036828, 0.028844293683889057)\n",
            "epochs 330 loss:  (0.014266308383109131, 0.028342179970457782)\n",
            "epochs 340 loss:  (0.013698128548735002, 0.028088231368183357)\n",
            "epochs 350 loss:  (0.013360671141770577, 0.028096421957544693)\n",
            "epochs 360 loss:  (0.013074875505364124, 0.028217816411624956)\n",
            "epochs 370 loss:  (0.01441390814180153, 0.030118032659508243)\n",
            "epochs 380 loss:  (0.17404331204247178, 0.19121628154835632)\n",
            "epochs 390 loss:  (0.03349008001631114, 0.05152668433499817)\n",
            "epochs 400 loss:  (26.767035856672734, 31.767320882061117)\n",
            "epochs 410 loss:  (20.783841975997746, 25.637158683413997)\n",
            "epochs 420 loss:  (7.5507833389368235, 12.323180241525376)\n",
            "epochs 430 loss:  (4.625710136703867, 9.367917509193308)\n",
            "epochs 440 loss:  (4.301526659557277, 9.027139410010129)\n",
            "epochs 450 loss:  (4.113641082442118, 8.825074679193683)\n",
            "epochs 460 loss:  (3.927473004740121, 8.62513582592354)\n",
            "epochs 470 loss:  (3.748936077613782, 8.433045937146018)\n",
            "epochs 480 loss:  (3.5782352395206534, 8.248982990165407)\n",
            "epochs 490 loss:  (3.415151721468393, 8.072716356474078)\n"
          ]
        }
      ],
      "source": [
        "y = one_hot_encode(train_Y, num_classes=10)\n",
        "layers = [128, 128, 10]\n",
        "\n",
        "activation_functions = ['relu', 'relu', 'softmax']\n",
        "hp = HyperParameters(layers=layers, learning_rate=0.1, epochs=500, mini_batch_size=2048, lambd=.1)\n",
        "nn = NeuralNetwork(train_X.T, y.T, hp, activation_functions=activation_functions)\n",
        "\n",
        "nn.train_nn( verbose=True, per_epoch_log=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Accuracy :**\n",
        "---\n",
        "---"
      ],
      "metadata": {
        "id": "RPv_oPbSV212"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CzD78q9Vkb0H",
        "outputId": "7208c656-6f80-4666-eb3e-927c70d5e1e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Accuracy of our MLP model is : 99.83%\n"
          ]
        }
      ],
      "source": [
        "print(f\"The Accuracy of our MLP model is : {np.count_nonzero(nn.predict(test_X.T) == test_Y)/100}%\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}