{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Answer-1**\n",
        "---\n",
        "---"
      ],
      "metadata": {
        "id": "57pV27G3VOXP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5gtPC5siQcvw",
        "outputId": "c1080f22-a626-4cae-8d3e-7d02e4703f00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.23.5\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "print(np.__version__)\n",
        "np.random.seed(200)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def Generator(n, m, theta = 0.0):\n",
        "    beta = np.random.randn(m+1, 1)\n",
        "    X = np.random.randn(n, m+1)\n",
        "    X[:, 0] = 1\n",
        "    h = 1 / (1 + np.exp(-np.matmul(X, beta)))\n",
        "    Y = np.round(h)\n",
        "    noise = np.random.binomial(n=1, p=theta, size = (len(Y), 1))\n",
        "    Y_noisy = np.logical_xor(Y, noise).astype(int)\n",
        "\n",
        "    return X, Y_noisy, beta"
      ],
      "metadata": {
        "id": "GpDPmRmPVWK7"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X, Y, beta = Generator(5, 5, theta=0.3)\n",
        "print(f\"X :{X}\\nY ={Y}\\n\\nβ ={beta}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C2HOpBg5Qpvb",
        "outputId": "d84ae9f6-2991-4aff-d5f3-819a9fc4ea62"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X :[[ 1.          1.27186338 -0.01231281 -0.26572854 -0.50239708 -0.03190136]\n",
            " [ 1.          0.97012715  1.18645564  0.1833277  -0.15419088  0.25202674]\n",
            " [ 1.          0.59786374 -0.95718825  1.04975291 -1.29226185  0.11316318]\n",
            " [ 1.         -0.59901089  2.01641332  0.1851378  -0.57764425 -0.1445562 ]\n",
            " [ 1.         -0.08129256 -0.46365774  0.01162035 -0.24131524  0.23495992]]\n",
            "Y =[[0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]]\n",
            "\n",
            "β =[[ 0.32714315]\n",
            " [ 0.58067026]\n",
            " [-0.12032434]\n",
            " [ 0.52964066]\n",
            " [-0.02615513]\n",
            " [ 0.26900105]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer-2**\n",
        "---\n",
        "---"
      ],
      "metadata": {
        "id": "orXr-kXlaw01"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Logistic_Regression(X, Y, k, t, alpha = 0.05):\n",
        "    beta = np.random.randn(X.shape[1], 1)\n",
        "    for i in range(k):\n",
        "        h = 1 / (1 + np.exp(-np.matmul(X, beta)))\n",
        "        cost = np.average(- Y * np.log(h) - (1 - Y) * np.log(1 - h))\n",
        "        gradient = np.matmul(X.T, (h - Y)) / len(Y)\n",
        "        beta -= alpha * gradient\n",
        "        new_h =  1 / (1 + np.exp(-np.matmul(X, beta)))\n",
        "        new_cost = np.average(- Y * np.log(new_h) - (1 - Y) * np.log(1 - new_h))\n",
        "        if np.abs(new_cost - cost) <= t:\n",
        "            break\n",
        "    return beta, new_cost"
      ],
      "metadata": {
        "id": "ON949UxaajJO"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Original parameters : \")\n",
        "print(f\"β :{beta}\\n\")\n",
        "print(\"Learned parameters : \")\n",
        "(beta,f_cost)=Logistic_Regression(X, Y, 10000, 0.0001, 0.9)\n",
        "print(f\"β :{beta}\\nThe final cost function value is : {f_cost}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JaxJA8P3b0X4",
        "outputId": "ffa4fc7e-4f9c-46cb-f04b-082be29e1dbc"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original parameters : \n",
            "β :[[ 3.68509465]\n",
            " [-4.60310549]\n",
            " [ 2.40816574]\n",
            " [ 4.31111469]\n",
            " [-0.90283285]\n",
            " [ 1.52595119]]\n",
            "\n",
            "Learned parameters : \n",
            "β :[[ 3.55583661]\n",
            " [-3.95697134]\n",
            " [ 1.94085857]\n",
            " [ 4.99602135]\n",
            " [ 0.12338401]\n",
            " [ 1.95500641]]\n",
            "The final cost function value is : 0.024769326753600655\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer-3**\n",
        "---\n",
        "---"
      ],
      "metadata": {
        "id": "7gRn2We4Y8IT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Impact of Dataset Size (n)** :\n",
        "\n",
        "Larger dataset sizes generally improve the logistic regression model's ability to learn coefficients β, leading to better predictive performance due to increased diversity and generalization capacity.\n",
        "\n",
        "**Impact of Label Noise (θ)** :\n",
        "\n",
        "Higher levels of label noise (θ) can hinder the model's learning process and degrade prediction accuracy, emphasizing the importance of clean, reliable data for effective logistic regression modeling.\n",
        "```\n",
        "The derivation of gradient of the cost function with respect to the parameters of the model is submitted through a scanned pdf in Edu-collab.\n",
        "```"
      ],
      "metadata": {
        "id": "jgFJZZblZA38"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer-4**\n",
        "---\n",
        "---"
      ],
      "metadata": {
        "id": "wy0Eiw2sY_zs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adding L1 and L2 regularization to the Logistic Regression cost function helps prevent overfitting by penalizing large coefficients. The impact of regularization on the learned models and the β vector depends on the choice of the regularization constant.\n",
        "\n",
        "**L1 Regularization (Lasso)** :\n",
        "L1 regularization adds the sum of the absolute values of the coefficients to the cost function. This encourages sparsity in the coefficients, as it tends to push some coefficients to zero.\n",
        "With higher values of the regularization constant (λ), more coefficients are likely to be pushed to zero, resulting in a simpler model with fewer features contributing to the prediction.\n",
        "The choice of λ balances between model simplicity (higher λ) and fitting the training data well (lower λ).\n",
        "\n",
        "**L2 Regularization (Ridge)** :\n",
        "\n",
        "L2 regularization adds the sum of the squared values of the coefficients to the cost function. It penalizes large coefficients but does not usually lead to sparsity in the coefficients. Increasing the regularization constant (λ) shrinks the coefficients towards zero, reducing the impact of individual features on the model's output. L2 regularization tends to distribute the weight more evenly among features compared to L1 regularization."
      ],
      "metadata": {
        "id": "1Wa4dV4Qf1YM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def logistic_regression_regularized(X, Y, k, t, alpha, reg = \"L1\"):\n",
        "    beta = np.random.randn(X.shape[1], 1)\n",
        "    lambda_reg  = 1\n",
        "    for i in range(k):\n",
        "        h = 1 / (1 + np.exp(-np.matmul(X, beta)))\n",
        "        if reg == \"L1\":\n",
        "            cost = np.average(- Y * np.log(h) - (1 - Y) * np.log(1 - h)) + lambda_reg * np.sum(np.abs(beta))\n",
        "            gradient = np.matmul(X.T, (h - Y)) / len(Y) + lambda_reg * np.sign(beta)\n",
        "        elif reg == \"L2\":\n",
        "            cost = np.average(- Y * np.log(h) - (1 - Y) * np.log(1 - h)) + lambda_reg * np.sum(np.square(beta))\n",
        "            gradient = np.matmul(X.T, (h - Y)) / len(Y) + lambda_reg * beta\n",
        "        beta -= alpha * gradient\n",
        "        new_h =  1 / (1 + np.exp(-np.matmul(X, beta)))\n",
        "        new_cost = np.average(- Y * np.log(new_h) - (1 - Y) * np.log(1 - new_h))\n",
        "        if np.abs(new_cost - cost) <= t:\n",
        "            break\n",
        "    return beta, new_cost\n",
        "\n",
        "print(\"Original parameters : \")\n",
        "print(f\"β :{beta}\\n\")\n",
        "print(\"Learned parameters : \")\n",
        "(new_beta1, cost1) = logistic_regression_regularized(X, Y, 100000, 0.0001, 0.01, reg=\"L1\")\n",
        "print(f\"β :{new_beta1}\\nThe final cost function value is : {cost1}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jvDEmQfNZExq",
        "outputId": "68a75ce6-dbe5-456b-f7af-0f0752a5ba08"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original parameters : \n",
            "β :[[ 3.55583661]\n",
            " [-3.95697134]\n",
            " [ 1.94085857]\n",
            " [ 4.99602135]\n",
            " [ 0.12338401]\n",
            " [ 1.95500641]]\n",
            "\n",
            "Learned parameters : \n",
            "β :[[ 0.00492722]\n",
            " [ 0.0005495 ]\n",
            " [-0.00795176]\n",
            " [-0.00791548]\n",
            " [-0.00508957]\n",
            " [ 0.0080309 ]]\n",
            "The final cost function value is : 0.6931830726171009\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The choice of the regularization constant (λ) impacts the β vector learned as follows** :\n",
        "\n",
        "For both L1 and L2 regularization, smaller values of λ result in less regularization, allowing the model to fit the training data more closely. This can lead to overfitting if λ is too small.\n",
        "\n",
        "Larger values of λ increase the amount of regularization, which can lead to simpler models with smaller coefficients. However, if λ is too large, the model may underfit the data.\n",
        "\n",
        "The optimal value of λ is usually determined through techniques like cross-validation, where different values of λ are tried and the one that yields the best performance on a validation set is selected."
      ],
      "metadata": {
        "id": "yciHH0QQhfAU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer-5**\n",
        "---\n",
        "---"
      ],
      "metadata": {
        "id": "g9vuTf2AeXFg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Regression:\n",
        "    def __init__(self, type=\"linear\"):\n",
        "        self.type = type\n",
        "\n",
        "    def train(self, X, Y, k, t, alpha=0.5):\n",
        "        beta = np.random.randn(X.shape[1], 1)\n",
        "        for i in range(k):\n",
        "            cost = self._cost(X, Y, beta)\n",
        "            gradient = self._gradient(X, Y, beta)\n",
        "            beta -= alpha * gradient\n",
        "            new_cost = self._cost(X, Y, beta)\n",
        "            if np.abs(new_cost - cost) <= t:\n",
        "                break\n",
        "\n",
        "        return beta\n",
        "\n",
        "    def _gradient(self, X, Y, beta):\n",
        "        if self.type == \"linear\":\n",
        "            error = np.matmul(X, beta) - Y\n",
        "            return X.T.dot(error)/len(Y)\n",
        "        elif self.type == \"logistic\":\n",
        "            h = 1 / (1 + np.exp(-np.matmul(X, beta)))\n",
        "            return np.matmul(X.T, (h - Y)) / len(Y)\n",
        "\n",
        "    def _cost(self, X, Y, beta):\n",
        "        if self.type == \"linear\":\n",
        "            error = np.matmul(X, beta) - Y\n",
        "            return np.sum(np.square(error))/(2*len(Y))\n",
        "        elif self.type == \"logistic\":\n",
        "            h = 1 / (1 + np.exp(-np.matmul(X, beta)))\n",
        "            return np.average(- Y * np.log(h) - (1 - Y) * np.log(1 - h))"
      ],
      "metadata": {
        "id": "WBWPKqKWeZ0j"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X, Y, beta = Generator(5, 5, theta=0.3)\n",
        "\n",
        "regression = Regression()\n",
        "regression.train(X,Y,1000,0.0001,alpha=0.5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sb06ukB2idA-",
        "outputId": "65ac994d-303a-4be5-d037-16e13eb61a6c"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1.04491996],\n",
              "       [ 0.35643135],\n",
              "       [-0.31211607],\n",
              "       [ 0.23966457],\n",
              "       [ 0.06084374],\n",
              "       [-0.06743868]])"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    }
  ]
}